# Module: `ai-feedback-generator.ts`

## 1. Module Summary

The `ai-feedback-generator` module provides GPT-5-Mini powered personalized feedback generation for daily task submissions, replacing template-based feedback with dynamic AI analysis. This module analyzes user answers in context of task requirements, generates detailed Traditional Chinese feedback with specific strengths/weaknesses/suggestions, implements answer truncation (500 chars) to avoid token limits, provides comprehensive console logging for debugging, and falls back to template feedback when AI is unavailable. The feedback follows a "ÂÑ™Èªû-‰∏çË∂≥-Âª∫Ë≠∞" (Strengths-Weaknesses-Suggestions) structure with encouraging tone.

**Phase 2.10 Update:** Module has been updated to properly handle GPT-5-mini's reasoning token behavior. The max_tokens parameter has been increased from 600 to 2000, timeout increased from 20s to 30s, and added verbosity/reasoning_effort parameters to ensure reliable feedback generation and prevent empty responses.

## 2. Module Dependencies

* **Internal Dependencies:**
  * `@/lib/openai-client` - GPT-5-Mini client for completion generation with fallback.
  * `@/lib/types/daily-task` - Type definitions for task types, difficulty, and task content.
* **External Dependencies:** None. Dependencies handled through openai-client.

## 3. Public API / Exports

* `generatePersonalizedFeedback(params: FeedbackGenerationParams): Promise<string>` - Generates AI-powered personalized feedback.
* **Type Exports:**
  * `FeedbackGenerationParams` - Parameters for feedback generation (taskType, userAnswer, score, difficulty, taskContent, taskTitle).

## 4. Code File Breakdown

### 4.1. `ai-feedback-generator.ts`

* **Purpose:** Transforms task evaluation from generic templates to personalized, context-aware analysis using GPT-5-Mini. The module's key innovations include: (1) **Context-rich prompts** - Includes task content (original text, questions, poems, characters) so AI understands what user was asked to do; (2) **Score-aware feedback** - AI knows the user's score (0-100) and can calibrate feedback intensity accordingly; (3) **Answer truncation strategy** - Limits user answers to 500 characters before sending to AI to avoid finish_reason="length" errors while preserving analysis quality; (4) **Comprehensive logging** - Outputs detailed terminal logs with emojis (üéì for start, ‚úÖ for success, ‚ùå for errors) showing answer preview, score, difficulty, and API response details; (5) **Multi-tier fallback** - Falls back to template feedback if AI times out, then to hardcoded fallback if template fails; (6) **Simplified prompts** - Uses concise prompts optimized for GPT-5-mini's capabilities, requesting 150-300 character feedback in specific structure.
* **Functions:**
    * `generatePersonalizedFeedback(params: FeedbackGenerationParams): Promise<string>` - **Main feedback generation function**. Logs detailed diagnostics with emojis to terminal. Truncates answers > 500 chars to avoid token limits. Builds task-specific prompt via `buildFeedbackPrompt`. Calls `generateCompletionWithFallback` from openai-client with 30s timeout (increased from 20s), 2000 max tokens (increased from 600 for GPT-5-mini reasoning), verbosity='medium', and reasoning_effort='minimal'. Generates template fallback via `generateTemplateFeedback`. Extracts feedback from result (handles both string and object responses). Falls back to hardcoded default if feedback empty. Logs success with model and token usage. Returns Traditional Chinese feedback string. Catches errors and returns fallback feedback.
    * `buildFeedbackPrompt(params: FeedbackGenerationParams): string` - Constructs AI prompt. Starts with role definition: "‰Ω†ÊòØ„ÄäÁ¥ÖÊ®ìÂ§¢„ÄãÊïôÂ∏´ÔºåË©ï‰º∞Â≠∏Áîü‰ΩúÁ≠î„ÄÇ" Adds task title and score. Calls `buildTaskSpecificContext` to add task content. Adds truncated user answer. Specifies output requirements: "Ë´ãÁî®ÁπÅÈ´î‰∏≠ÊñáÂØ´150-300Â≠óË©ïË™ûÔºåÂåÖÂê´Ôºö1. ÂÑ™ÈªûÔºàÂÖ∑È´îË™™ÊòéÂ•ΩÁöÑÂú∞ÊñπÔºâ2. ‰∏çË∂≥ÔºàÈúÄË¶ÅÊîπÈÄ≤ÁöÑÂú∞ÊñπÔºâ3. Âª∫Ë≠∞ÔºàÂ¶Ç‰ΩïÈÄ≤Ê≠•Ôºâ". Returns complete prompt string.
    * `buildTaskSpecificContext(taskType, taskContent): string` - Adds task-specific context based on type. MORNING_READING: includes original text passage and question. POETRY: includes poem title and content. CHARACTER_INSIGHT: includes character name and context. CULTURAL_EXPLORATION: includes cultural element title, description, and first question. COMMENTARY_DECODE: includes commentary text and original text. Returns context string for AI understanding.
    * `getTaskTypeDisplayName(taskType): string` - Maps task type enum to Traditional Chinese display names. Returns strings like "Êô®ËÆÄÊôÇÂÖâ - ÊñáÊú¨ÁêÜËß£", "Ë©©Ë©ûÈüªÂæã - Ë©©Ë©ûÈªòÂØ´", "‰∫∫Áâ©Ê¥ûÂØü - ËßíËâ≤ÂàÜÊûê", etc.
    * `getDifficultyDisplayName(difficulty): string` - Maps difficulty enum to Traditional Chinese. EASY ‚Üí "Á∞°ÂñÆ", MEDIUM ‚Üí "‰∏≠Á≠â", HARD ‚Üí "Âõ∞Èõ£".
    * `generateTemplateFeedback(taskType, score): string` - **Fallback template generator**. Categorizes score: >= 85 = excellent, >= 70 = good, >= 60 = average, < 60 = needsWork. Each category has 3 randomized feedback templates in Traditional Chinese following "ÂÑ™Èªû-‰∏çË∂≥-Âª∫Ë≠∞" structure with encouraging "ÁπºÁ∫åÂä†Ê≤πÔºÅ" endings. Examples: "ÂÑ™ÈªûÔºöÂàÜÊûêÊ∑±ÂÖ•‰∏îÊ¢ùÁêÜÊ∏ÖÊ•ö„ÄÇ‰∏çË∂≥Ôºö‰ªçÂèØË£úÂÖÖÈóúÈçµË™ûÂè•ÂºïÁî®„ÄÇÂª∫Ë≠∞ÔºöÁ∂≠ÊåÅÈÄô‰ªΩÁ¥∞Á∑ªÂ∫¶‰∏¶ÁπºÁ∫åÊì¥Â±ïËßÄÈªûÔºåÁπºÁ∫åÂä†Ê≤πÔºÅ" Returns random template from appropriate category.
* **Key Classes / Constants / Variables:**
    * `FeedbackGenerationParams: interface` - 6 fields: taskType (DailyTaskType enum), userAnswer (string), score (number 0-100), difficulty (TaskDifficulty enum), taskContent (task-specific content object), taskTitle (optional string).
    * Feedback templates - 4 categories (excellent/good/average/needsWork), each with 3 variations, all in Traditional Chinese with encouraging tone and specific improvement suggestions.

## 5. System and Data Flow

### 5.1. System Flowchart (Control Flow)

```mermaid
flowchart TD
    A[Start: generatePersonalizedFeedback] --> B[Log diagnostics to terminal];
    B --> C{User answer > 500 chars?};
    C -- Yes --> D[Truncate to 500 chars + warning];
    C -- No --> E[Use full answer];
    D --> F[Build AI prompt with context];
    E --> F;
    F --> G[Generate template fallback];
    G --> H[Call GPT-5-Mini with 30s timeout, 2000 max_tokens];
    H --> I{AI response received?};
    I -- Yes --> J[Extract feedback text];
    I -- Timeout/Error --> K[Use template fallback];
    J --> L{Feedback non-empty?};
    L -- Yes --> M[Return AI feedback];
    L -- No --> N[Return hardcoded fallback];
    K --> M;
    M --> O[Log success with details];
    N --> O;
    O --> P[End];
```

### 5.2. Data Flow Diagram (Data Transformation)

```mermaid
graph LR
    Input1(Task Type, Score, Difficulty) --> Context[Build task-specific context];
    Input2(User Answer) --> Truncate[Truncate if > 500 chars];
    Input3(Task Content: passage/poem/character) --> Context;
    Context --> Prompt[Construct AI prompt];
    Truncate --> Prompt;
    Prompt --> AI[GPT-5-Mini API];
    AI --> Parse[Extract feedback text];
    Parse --> Validate{Non-empty?};
    Validate -- Yes --> Output(Personalized Traditional Chinese Feedback);
    Validate -- No --> Fallback[Template Feedback];
    Fallback --> Output;
```

## 6. Usage Example & Testing

* **Usage:**
```typescript
import { generatePersonalizedFeedback } from '@/lib/ai-feedback-generator';
import { DailyTaskType, TaskDifficulty } from './types/daily-task';

// Generate feedback for morning reading task
const feedback = await generatePersonalizedFeedback({
  taskType: DailyTaskType.MORNING_READING,
  userAnswer: 'ÊûóÈªõÁéâÂàùÂÖ•Ë≥àÂ∫úÊôÇÂ±ïÁèæÂá∫Ê•µÂ∫¶Ë¨πÊÖéÁöÑÂøÉÁêÜÁãÄÊÖãÔºåÂéüÊñá‰∏≠„ÄåÊ≠•Ê≠•ÁïôÂøÉÔºåÊôÇÊôÇÂú®ÊÑè„ÄçÂÖÖÂàÜÈ´îÁèæ‰∫ÜÂ•π‰ΩúÁÇ∫ÂØÑ‰∫∫Á±¨‰∏ãËÄÖÁöÑÊïèÊÑüËàáËá™Êàë‰øùË≠∑ÊÑèË≠ò„ÄÇ',
  score: 85,
  difficulty: TaskDifficulty.MEDIUM,
  taskContent: {
    textPassage: {
      text: 'ÂçªË™™ÈªõÁéâËá™ÈÇ£Êó•Ê£ÑËàüÁôªÂ≤∏ÊôÇ...',
      question: 'ÂæûÈÄôÊÆµÊñáÂ≠ó‰∏≠Ôºå‰Ω†Ë™çÁÇ∫ÊûóÈªõÁéâÂàùÂà∞Ë≥àÂ∫úÊôÇÁöÑÂøÉÁêÜÁãÄÊÖãÊòØ‰ªÄÈ∫ºÔºü',
      hint: 'ÊÄùËÄÉÊèêÁ§∫ÔºöÊ≥®ÊÑèÈªõÁéâÁöÑË°åÁÇ∫ÊèèËø∞...',
      expectedKeywords: ['Ë¨πÊÖé', 'Â∞èÂøÉ', 'ÊãòË¨π', 'ÂØÑ‰∫∫Á±¨‰∏ã']
    }
  },
  taskTitle: 'Êô®ËÆÄÊôÇÂÖâ - Ê∑±ÂÖ•Á¥ÖÊ®ì'
});

console.log(feedback);
// Output example: "ÂÑ™ÈªûÔºöÊÇ®Ê∫ñÁ¢∫Êäì‰Ωè‰∫ÜÊûóÈªõÁéâ„ÄåÊ≠•Ê≠•ÁïôÂøÉÔºåÊôÇÊôÇÂú®ÊÑè„ÄçÁöÑÊ†∏ÂøÉÂøÉÁêÜÔºå‰∏¶Ê≠£Á¢∫ËÅØÁπ´Âà∞„ÄåÂØÑ‰∫∫Á±¨‰∏ã„ÄçÁöÑËôïÂ¢ÉÔºå
// Â±ïÁèæÂá∫ËâØÂ•ΩÁöÑÊñáÊú¨ÁêÜËß£ËÉΩÂäõ„ÄÇ‰∏çË∂≥ÔºöÂàÜÊûêÁï•È°ØÁ∞°Áï•ÔºåÂèØ‰ª•ÈÄ≤‰∏ÄÊ≠•Êé¢Ë®éÂ•πÈÄôÁ®ÆÂøÉÁêÜÂ∞çÂæåÁ∫åÊÄßÊ†ºÂ°ëÈÄ†ÁöÑÂΩ±Èüø„ÄÇ
// Âª∫Ë≠∞ÔºöÂòóË©¶ÂæûÊõ¥Â§öÂéüÊñáÁ¥∞ÁØÄÂá∫ÁôºÔºåË£úÂÖÖÂÖ∑È´îË°åÁÇ∫ÊèèÂØ´ÔºåËÆìÂàÜÊûêÊõ¥Âä†Ë±êÊªø„ÄÇÁπºÁ∫å‰øùÊåÅÈÄô‰ªΩÊïèÈä≥ÁöÑËßÄÂØüÂäõÔºÅ"

// Long answer truncation example
const longAnswer = 'ÊûóÈªõÁéâÂàùÂÖ•Ë≥àÂ∫úÁöÑÂøÉÁêÜÁãÄÊÖã...' + '...'.repeat(200); // Very long answer
const feedbackTruncated = await generatePersonalizedFeedback({
  taskType: DailyTaskType.CHARACTER_INSIGHT,
  userAnswer: longAnswer, // Will be truncated to 500 chars
  score: 75,
  difficulty: TaskDifficulty.HARD,
  taskContent: {
    character: {
      characterName: 'ÊûóÈªõÁéâ',
      description: 'ÊûóÈªõÁéâÊòØË≥àÊØçÁöÑÂ§ñÂ≠´Â•≥...',
      analysisPrompts: ['ÊûóÈªõÁéâÁöÑÊÄßÊ†ºÁâπÈªûÊòØ‰ªÄÈ∫ºÔºü']
    }
  }
});
// Console will show: "‚ö†Ô∏è [Feedback] Â≠∏ÁîüÁ≠îÊ°àÈÅéÈï∑ (XXX Â≠óÂÖÉ)ÔºåÂ∑≤Êà™Êñ∑Ëá≥ 500 Â≠óÂÖÉ‰ª•ÁØÄÁúÅ tokens"

// Fallback scenario (AI unavailable)
process.env.OPENAI_API_KEY = ''; // Simulate AI unavailable
const feedbackFallback = await generatePersonalizedFeedback({
  taskType: DailyTaskType.POETRY,
  userAnswer: 'ÈÄôÈ¶ñË©©ÊèèÂØ´‰∫ÜÁßãÂ§©ÁöÑÊôØËâ≤',
  score: 65,
  difficulty: TaskDifficulty.EASY,
  taskContent: { poem: { title: 'ÁßãÁ™óÈ¢®Èõ®Â§ï', content: 'ÁßãËä±ÊÖòÊ∑°ÁßãËçâÈªÉ...' } }
});
// Returns template feedback from excellent/good/average/needsWork categories
```
* **Testing:** Testing strategy focuses on prompt generation and fallback behavior:
  - Test feedback generation with valid AI response returns personalized feedback
  - Test answer truncation when > 500 characters
  - Test truncation preserves answer meaning for short content
  - Test prompt includes task-specific context (text/poem/character)
  - Test prompt includes score for AI calibration
  - Test feedback format follows "ÂÑ™Èªû-‰∏çË∂≥-Âª∫Ë≠∞" structure
  - Test Traditional Chinese output
  - Test template fallback when AI timeout occurs
  - Test hardcoded fallback when both AI and template fail
  - Test different score ranges map to appropriate template categories
  - Test console logging outputs expected format with emojis
  - Test empty answer handling
  - Test all task types have appropriate context building
  - Test model and token usage are logged when AI succeeds
