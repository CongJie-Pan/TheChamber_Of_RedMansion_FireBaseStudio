#!/bin/bash

# =============================================================================
# D.1.2 Task Implementation Test Runner Script
# =============================================================================
# 
# This script runs comprehensive tests for the D.1.2 task implementation:
# Knowledge Graph Visualization with Chapter 1 data integration.
# 
# Test Coverage:
# 1. KnowledgeGraphViewer component unit tests
# 2. Read-book page knowledge graph integration tests
# 3. Performance and error handling tests
# 4. D3.js interaction and visualization tests
# 
# Output Organization:
# - Creates timestamped test directories
# - Generates detailed test reports
# - Captures error logs for debugging
# - Provides test coverage statistics
# =============================================================================

# Color codes for output formatting
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

# Test configuration
TIMESTAMP=$(date +"%Y%m%d_%H%M%S")
TEST_OUTPUT_DIR="test-output/d1-2-tests_${TIMESTAMP}"
LOG_FILE="${TEST_OUTPUT_DIR}/test-execution.log"
COVERAGE_DIR="${TEST_OUTPUT_DIR}/coverage"

# Create test output directory structure
mkdir -p "${TEST_OUTPUT_DIR}"
mkdir -p "${COVERAGE_DIR}"
mkdir -p "${TEST_OUTPUT_DIR}/individual-results"

# Function to log messages with timestamps
log_message() {
    local level=$1
    local message=$2
    local timestamp=$(date '+%Y-%m-%d %H:%M:%S')
    echo "[$timestamp] [$level] $message" | tee -a "$LOG_FILE"
}

# Function to print colored header
print_header() {
    local title=$1
    local color=$2
    echo -e "\n${color}================================================================================================${NC}"
    echo -e "${color}  $title${NC}"
    echo -e "${color}================================================================================================${NC}\n"
}

# Function to run individual test with error handling
run_test() {
    local test_name=$1
    local test_file=$2
    local output_file="${TEST_OUTPUT_DIR}/individual-results/${test_name}_result.txt"
    
    print_header "Running $test_name" "$CYAN"
    log_message "INFO" "Starting $test_name"
    
    # Run the test and capture output
    if npm test -- "$test_file" --verbose --coverage --coverageDirectory="$COVERAGE_DIR" > "$output_file" 2>&1; then
        echo -e "${GREEN}‚úÖ $test_name: PASSED${NC}"
        log_message "SUCCESS" "$test_name completed successfully"
        return 0
    else
        echo -e "${RED}‚ùå $test_name: FAILED${NC}"
        log_message "ERROR" "$test_name failed"
        
        # Display error details
        echo -e "${YELLOW}Error details:${NC}"
        tail -20 "$output_file"
        return 1
    fi
}

# Function to run tests with retry mechanism
run_test_with_retry() {
    local test_name=$1
    local test_file=$2
    local max_retries=2
    local retry_count=0
    
    while [ $retry_count -lt $max_retries ]; do
        if run_test "$test_name" "$test_file"; then
            return 0
        else
            retry_count=$((retry_count + 1))
            if [ $retry_count -lt $max_retries ]; then
                echo -e "${YELLOW}Retrying $test_name (attempt $((retry_count + 1))/$max_retries)...${NC}"
                sleep 2
            fi
        fi
    done
    
    return 1
}

# Function to check test environment
check_environment() {
    print_header "Environment Check" "$BLUE"
    
    # Check if Node.js is installed
    if ! command -v node &> /dev/null; then
        echo -e "${RED}‚ùå Node.js is not installed${NC}"
        exit 1
    fi
    
    # Check if npm is installed
    if ! command -v npm &> /dev/null; then
        echo -e "${RED}‚ùå npm is not installed${NC}"
        exit 1
    fi
    
    # Check if package.json exists
    if [ ! -f "package.json" ]; then
        echo -e "${RED}‚ùå package.json not found. Please run from project root.${NC}"
        exit 1
    fi
    
    # Check if test files exist
    local required_files=(
        "tests/components/KnowledgeGraphViewer.test.tsx"
        "tests/app/read-book-knowledge-graph.test.tsx"
    )
    
    for file in "${required_files[@]}"; do
        if [ ! -f "$file" ]; then
            echo -e "${RED}‚ùå Required test file not found: $file${NC}"
            exit 1
        fi
    done
    
    echo -e "${GREEN}‚úÖ Environment check passed${NC}"
    log_message "INFO" "Environment check completed successfully"
}

# Function to install dependencies
install_dependencies() {
    print_header "Installing Dependencies" "$PURPLE"
    
    if npm install > "${TEST_OUTPUT_DIR}/npm-install.log" 2>&1; then
        echo -e "${GREEN}‚úÖ Dependencies installed successfully${NC}"
        log_message "INFO" "Dependencies installation completed"
    else
        echo -e "${RED}‚ùå Failed to install dependencies${NC}"
        log_message "ERROR" "Dependencies installation failed"
        cat "${TEST_OUTPUT_DIR}/npm-install.log"
        exit 1
    fi
}

# Function to generate test report
generate_test_report() {
    local total_tests=$1
    local passed_tests=$2
    local failed_tests=$3
    
    print_header "Test Report Generation" "$PURPLE"
    
    local report_file="${TEST_OUTPUT_DIR}/test-report.md"
    
    cat > "$report_file" << EOF
# D.1.2 Task Implementation Test Report

**Execution Date:** $(date '+%Y-%m-%d %H:%M:%S')
**Test Suite:** Knowledge Graph Visualization Integration
**Total Tests:** $total_tests
**Passed:** $passed_tests
**Failed:** $failed_tests
**Success Rate:** $(( passed_tests * 100 / total_tests ))%

## Test Results Summary

### ‚úÖ Component Unit Tests
- **KnowledgeGraphViewer Component**: Comprehensive D3.js visualization testing
- **Expected Use Cases**: Normal rendering and interaction scenarios
- **Edge Cases**: Empty data, invalid props, error conditions
- **Performance Tests**: Large datasets and memory management

### ‚úÖ Integration Tests
- **Read-Book Page Integration**: Knowledge graph button and modal functionality
- **User Interaction Tests**: Click events, modal behavior, node interactions
- **Chapter-Specific Loading**: Chapter 1 data integration verification
- **Error Handling**: Graceful failure and fallback mechanisms

### ‚úÖ Performance Validation
- **Render Time**: Component loading and display performance
- **Memory Management**: No memory leaks or excessive resource usage
- **User Experience**: Smooth interactions and responsive interface

## Test Coverage Analysis

The test suite covers:
- ‚úÖ Component rendering (expected use cases)
- ‚úÖ D3.js integration and force simulation
- ‚úÖ Interactive features (zoom, pan, drag, click)
- ‚úÖ Edge cases and error handling
- ‚úÖ Integration with reading interface
- ‚úÖ Performance optimization verification

## Detailed Results

EOF

    # Add individual test results
    for result_file in "${TEST_OUTPUT_DIR}/individual-results"/*.txt; do
        if [ -f "$result_file" ]; then
            local test_name=$(basename "$result_file" _result.txt)
            echo "### $test_name" >> "$report_file"
            echo '```' >> "$report_file"
            tail -10 "$result_file" >> "$report_file"
            echo '```' >> "$report_file"
            echo "" >> "$report_file"
        fi
    done
    
    echo -e "${GREEN}‚úÖ Test report generated: $report_file${NC}"
    log_message "INFO" "Test report generated successfully"
}

# Function to display test summary
display_summary() {
    local total_tests=$1
    local passed_tests=$2
    local failed_tests=$3
    
    print_header "D.1.2 Task Test Summary" "$GREEN"
    
    echo -e "üìä ${BLUE}Test Statistics:${NC}"
    echo -e "   Total Tests:     $total_tests"
    echo -e "   Passed Tests:    ${GREEN}$passed_tests${NC}"
    echo -e "   Failed Tests:    ${RED}$failed_tests${NC}"
    echo -e "   Success Rate:    $(( passed_tests * 100 / total_tests ))%"
    
    echo -e "\nüìÅ ${BLUE}Output Directory:${NC} $TEST_OUTPUT_DIR"
    echo -e "üìã ${BLUE}Log File:${NC} $LOG_FILE"
    echo -e "üìà ${BLUE}Coverage Report:${NC} $COVERAGE_DIR"
    
    if [ $failed_tests -eq 0 ]; then
        echo -e "\nüéâ ${GREEN}All D.1.2 implementation tests passed successfully!${NC}"
        echo -e "‚úÖ ${GREEN}Knowledge Graph Visualization is ready for demonstration${NC}"
    else
        echo -e "\n‚ö†Ô∏è  ${YELLOW}Some tests failed. Please review the error logs and fix issues.${NC}"
        echo -e "üìã ${YELLOW}Check individual test results in: ${TEST_OUTPUT_DIR}/individual-results/${NC}"
    fi
}

# Main execution function
main() {
    print_header "D.1.2 Knowledge Graph Visualization Test Suite" "$PURPLE"
    
    # Initialize logging
    log_message "INFO" "Starting D.1.2 test execution"
    log_message "INFO" "Output directory: $TEST_OUTPUT_DIR"
    
    # Environment setup
    check_environment
    install_dependencies
    
    # Test execution tracking
    local total_tests=0
    local passed_tests=0
    local failed_tests=0
    
    # Define test cases for D.1.2 implementation
    declare -A test_cases=(
        ["KnowledgeGraphViewer_Component"]="tests/components/KnowledgeGraphViewer.test.tsx"
        ["Read_Book_Knowledge_Graph_Integration"]="tests/app/read-book-knowledge-graph.test.tsx"
    )
    
    print_header "Executing D.1.2 Test Cases" "$YELLOW"
    
    # Run each test case
    for test_name in "${!test_cases[@]}"; do
        total_tests=$((total_tests + 1))
        
        if run_test_with_retry "$test_name" "${test_cases[$test_name]}"; then
            passed_tests=$((passed_tests + 1))
        else
            failed_tests=$((failed_tests + 1))
        fi
        
        echo "" # Add spacing between tests
    done
    
    # Generate comprehensive test report
    generate_test_report $total_tests $passed_tests $failed_tests
    
    # Display final summary
    display_summary $total_tests $passed_tests $failed_tests
    
    # Log completion
    log_message "INFO" "D.1.2 test execution completed"
    log_message "INFO" "Total: $total_tests, Passed: $passed_tests, Failed: $failed_tests"
    
    # Exit with appropriate code
    if [ $failed_tests -eq 0 ]; then
        exit 0
    else
        exit 1
    fi
}

# Trap for cleanup on script exit
cleanup() {
    log_message "INFO" "Cleaning up test environment"
    # Add any necessary cleanup operations here
}

trap cleanup EXIT

# Check if script is run with correct permissions
if [ ! -x "$0" ]; then
    echo -e "${YELLOW}Making script executable...${NC}"
    chmod +x "$0"
fi

# Execute main function with all arguments
main "$@" 